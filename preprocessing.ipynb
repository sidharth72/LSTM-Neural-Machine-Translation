{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "df = pd.read_csv('/teamspace/studios/this_studio/Translator/data/Hindi_English_Truncated_Corpus (1).csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8996152071804627"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_word_ratio(df, english_column, hindi_column):\n",
    "    english_words = df[english_column].apply(lambda x: len(x.split()))\n",
    "    hindi_words = df[hindi_column].apply(lambda x: len(x.split()))\n",
    "\n",
    "    return english_words.mean() / hindi_words.mean()\n",
    "\n",
    "calculate_word_ratio(df, 'english_sentence', 'hindi_sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average English Sequence Length: 16.091814584068022\n",
      "Average Hindi Sequence Length: 17.887441714666352\n"
     ]
    }
   ],
   "source": [
    "df['english_count'] = df['english_sentence'].apply(lambda x: len(x.split()))\n",
    "df['hindi_count'] = df['hindi_sentence'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Average English Sequence Length:\", df['english_count'].mean())\n",
    "print(\"Average Hindi Sequence Length:\", df['hindi_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196207</th>\n",
       "      <td>and put it in our cheeks.</td>\n",
       "      <td>और अपने गालों में डाल लेते हैं।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196208</th>\n",
       "      <td>As for the other derivatives of sulphur , the ...</td>\n",
       "      <td>जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196209</th>\n",
       "      <td>, etc . sulphates were limited , and the produ...</td>\n",
       "      <td>सल्फेट आदि की आवश्यकता सीमित थी और युद्धोपरांत...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196210</th>\n",
       "      <td>its complicated functioning is defined thus in...</td>\n",
       "      <td>Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196211</th>\n",
       "      <td>They've just won four government contracts to ...</td>\n",
       "      <td>हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196212 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         english_sentence  \\\n",
       "0       politicians do not have permission to do what ...   \n",
       "1              I'd like to tell you about one such child,   \n",
       "2       This percentage is even greater than the perce...   \n",
       "3       what we really mean is that they're bad at not...   \n",
       "4       .The ending portion of these Vedas is called U...   \n",
       "...                                                   ...   \n",
       "196207                          and put it in our cheeks.   \n",
       "196208  As for the other derivatives of sulphur , the ...   \n",
       "196209  , etc . sulphates were limited , and the produ...   \n",
       "196210  its complicated functioning is defined thus in...   \n",
       "196211  They've just won four government contracts to ...   \n",
       "\n",
       "                                           hindi_sentence  \n",
       "0       राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1       मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2        यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3          हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4             इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  \n",
       "...                                                   ...  \n",
       "196207                    और अपने गालों में डाल लेते हैं।  \n",
       "196208  जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...  \n",
       "196209  सल्फेट आदि की आवश्यकता सीमित थी और युद्धोपरांत...  \n",
       "196210  Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .  \n",
       "196211  हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...  \n",
       "\n",
       "[196212 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_buckets(df, seq_length, tolerance_ratio=1.2):\n",
    "    \"\"\"\n",
    "    Create buckets of sequences of constant size for machine translation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with columns 'english_sentence' and 'hindi_sentence'.\n",
    "        seq_length (int): Desired sequence length for each bucket.\n",
    "        tolerance_ratio (float): Tolerance ratio for the difference in sequence length between English and Hindi.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Modified dataframe with buckets of the specified sequence length.\n",
    "    \"\"\"\n",
    "    english_buckets = []\n",
    "    hindi_buckets = []\n",
    "\n",
    "    # Iterate over the dataframe rows\n",
    "    for idx, row in df.iterrows():\n",
    "        english_tokens = row['english_sentence'].split()\n",
    "        hindi_tokens = row['hindi_sentence'].split()\n",
    "\n",
    "        english_len = len(english_tokens)\n",
    "        hindi_len = len(hindi_tokens)\n",
    "\n",
    "        i = 0\n",
    "        while i < max(english_len, hindi_len):\n",
    "            english_bucket = english_tokens[i:i+seq_length]\n",
    "            hindi_bucket = hindi_tokens[i:i+int(seq_length * tolerance_ratio)]\n",
    "\n",
    "            # Append the bucket to the list\n",
    "            english_buckets.append(' '.join(english_bucket))\n",
    "            hindi_buckets.append(' '.join(hindi_bucket))\n",
    "\n",
    "            # Move to the next bucket\n",
    "            i += seq_length\n",
    "\n",
    "    # Create the new dataframe\n",
    "    bucketed_df = pd.DataFrame({\n",
    "        'english_sentence': english_buckets,\n",
    "        'hindi_sentence': hindi_buckets\n",
    "    })\n",
    "\n",
    "    return bucketed_df\n",
    "\n",
    "df = create_buckets(df, 18)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_english(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'\\d+', '', sentence)\n",
    "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def preprocess_hindi(sentence):\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "\n",
    "df['english_sentence'] = df['english_sentence'].apply(preprocess_english)\n",
    "df['hindi_sentence'] = df['hindi_sentence'].apply(preprocess_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "english_sentences = df['english_sentence'].tolist()\n",
    "hindi_sentences = df['hindi_sentence'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab_mapping = dict()\n",
    "hindi_vocab_mapping = dict()\n",
    "\n",
    "def generate_vocab_map(d, sentences):\n",
    "    index = 1\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            if word not in d:\n",
    "                d[word] = index\n",
    "                index += 1\n",
    "\n",
    "generate_vocab_map(english_vocab_mapping, english_sentences)\n",
    "generate_vocab_map(hindi_vocab_mapping, hindi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 70352\n",
      "Hindi vocabulary size: 93921\n"
     ]
    }
   ],
   "source": [
    "print(\"English vocabulary size:\", len(english_vocab_mapping))\n",
    "print(\"Hindi vocabulary size:\", len(hindi_vocab_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokens = []\n",
    "hindi_tokens = []\n",
    "\n",
    "def generate_tokens(tokens_list, sentences, vocab_map):\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = []\n",
    "        for word in sentence.split():\n",
    "            if word in vocab_map:\n",
    "                sentence_tokens.append(vocab_map[word])\n",
    "\n",
    "        tokens_list.append(sentence_tokens)\n",
    "\n",
    "generate_tokens(english_tokens, english_sentences, english_vocab_mapping)\n",
    "generate_tokens(hindi_tokens, hindi_sentences, hindi_vocab_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int_to_text(int_sequences, vocab_map):\n",
    "    # Create a reverse mapping from index to word\n",
    "    reverse_vocab_map = {index: word for word, index in vocab_map.items()}\n",
    "\n",
    "    text_sequences = []\n",
    "    for int_sequence in int_sequences:\n",
    "        text_sequence = []\n",
    "        for token in int_sequence:\n",
    "            if token in reverse_vocab_map:\n",
    "                text_sequence.append(reverse_vocab_map[token])\n",
    "\n",
    "        text_sequences.append(' '.join(text_sequence))\n",
    "\n",
    "    return text_sequences\n",
    "\n",
    "int_to_text = convert_int_to_text(english_tokens, english_vocab_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 18\n",
    "\n",
    "def generate_padding_tokens(tokens_list, sequence_length):\n",
    "    padded_tokens_list = []\n",
    "    for tokens in tokens_list:\n",
    "        if len(tokens) < sequence_length:\n",
    "            tokens = tokens + [0] * (sequence_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:sequence_length]\n",
    "\n",
    "        padded_tokens_list.append(tokens)\n",
    "\n",
    "    return padded_tokens_list\n",
    "\n",
    "padded_english_tokens = generate_padding_tokens(english_tokens, sequence_length)\n",
    "padded_hindi_tokens = generate_padding_tokens(hindi_tokens, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decoder_targets(padded_tokens_list, sequence_length):\n",
    "    decoder_targets = []\n",
    "    for tokens in padded_tokens_list:\n",
    "        if len(tokens) < sequence_length:\n",
    "            shifted_tokens = tokens[1:] + [0]\n",
    "        else:\n",
    "            shifted_tokens = tokens[1:sequence_length] + [0]\n",
    "        decoder_targets.append(shifted_tokens)\n",
    "    return decoder_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = np.array(padded_english_tokens)\n",
    "tgt = np.array(padded_hindi_tokens)\n",
    "labels = np.array(generate_decoder_targets(padded_hindi_tokens, sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2,     3, ...,     0,     0,     0],\n",
       "       [   11,    12,     6, ...,     0,     0,     0],\n",
       "       [   19,    20,    21, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 2657, 70352,   119, ...,     0,     0,     0],\n",
       "       [   74, 15678,  5136, ...,     0,     0,     0],\n",
       "       [ 8045,  2171,  4055, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2,     3, ...,    16,     0,     0],\n",
       "       [    1,    17,    18, ...,     0,     0,     0],\n",
       "       [    1,    27,    28, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    1, 10399,   739, ...,    57,    15,    16],\n",
       "       [    1, 93920,   142, ...,     0,     0,     0],\n",
       "       [    1,    88,    20, ...,    16,     0,     0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    2,     3,     4, ...,     0,     0,     0],\n",
       "       [   17,    18,    19, ...,     0,     0,     0],\n",
       "       [   27,    28,    29, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [10399,   739,    61, ...,    15,    16,     0],\n",
       "       [93920,   142,  1153, ...,     0,     0,     0],\n",
       "       [   88,    20,    24, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/teamspace/studios/this_studio/Translator/data/dataset.npz', src = src, tgt=tgt, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "\n",
    "with open('/teamspace/studios/this_studio/Translator/data/src_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(english_vocab_mapping, f)\n",
    "\n",
    "with open('/teamspace/studios/this_studio/Translator/data/tgt_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(hindi_vocab_mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
